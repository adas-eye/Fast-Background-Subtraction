{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " BGS on Synthetic dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ud21sjiQ19QD",
        "HDMcR-S01B2b",
        "zhYxKZ9_7i9K",
        "29VMnQ9l2HjX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud21sjiQ19QD"
      },
      "source": [
        "# Libraries install if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbpBE5vHHw62"
      },
      "source": [
        "# !pip install --pre torch -f  https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrCKsyyWICFw"
      },
      "source": [
        "# !pip uninstall --y torch torchvision\n",
        "# !pip install --pre torchvision -f  https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDMcR-S01B2b"
      },
      "source": [
        "# COCO DATASET DOWNLOAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8H3nWRj1XSO"
      },
      "source": [
        "I have tried working directly with zip file using python library `zipFile`.\r\n",
        "\r\n",
        "It works fine with single thread, but on multithread the library throws exception.\r\n",
        "\r\n",
        "I have tried to find solution but no luck. A discussion is at : [torch forums](https://discuss.pytorch.org/t/dataloader-with-zipfile-failed/42795)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTnCPSff2BJu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Inwsls87s8o"
      },
      "source": [
        "#---------------------\r\n",
        "# download COCO dataset\r\n",
        "# uncomment and run only if necessary\r\n",
        "#---------------------\r\n",
        "\r\n",
        "# download dataset to drive\r\n",
        "# https://stackoverflow.com/questions/55556965/importing-coco-datasets-to-google-colaboratory\r\n",
        "\r\n",
        "# !wget http://images.cocodataset.org/zips/train2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDtGBEWHn8L9"
      },
      "source": [
        "# !unzip -q train2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALIIh3dr73hV",
        "outputId": "4c9b5192-d43d-4038-fb0a-d75d09a7fdf6"
      },
      "source": [
        "# !wget http://images.cocodataset.org/zips/val2017.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-14 11:35:25--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.96.147\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.96.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  46.1MB/s    in 17s     \n",
            "\n",
            "2020-12-14 11:35:42 (44.9 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH4Tr7SQpipu"
      },
      "source": [
        "# !unzip -q val2017.zip"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhYxKZ9_7i9K"
      },
      "source": [
        "# CDNET DATASET DOWNLOAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtxKnAE-xsFt"
      },
      "source": [
        "#-----------\n",
        "# untar\n",
        "#---------\n",
        "# !tar xvf /content/drive/My\\ Drive/Projects/BGS/dynamic.tar -C /content/drive/My\\ Drive/Projects/BGS/DATA/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29VMnQ9l2HjX"
      },
      "source": [
        "# Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxwx_HnK2K63"
      },
      "source": [
        "import os \r\n",
        "class GlobalVars:\r\n",
        "\r\n",
        "    #-----------------\r\n",
        "    # DATASET RELATED\r\n",
        "    #-----------------\r\n",
        "\r\n",
        "    PROJECT_FOLDER = '/content/drive/My Drive/Projects/BGS/'\r\n",
        "\r\n",
        "    # COCO DATASET\r\n",
        "    COCOPath = os.path.join ( PROJECT_FOLDER, \"COCO\" ) \r\n",
        "\r\n",
        "    trainUnZipFilePath = \"train2017\"\r\n",
        "    valUnZipFilePath = \"val2017\"\r\n",
        "    trainZipFilePath = trainUnZipFilePath + \".zip\"\r\n",
        "    valZipFilePath = valUnZipFilePath + \".zip\" \r\n",
        "\r\n",
        "    # CDNET DATASET\r\n",
        "    CDNETPath = os.path.join ( PROJECT_FOLDER, \"CDNET\" )\r\n",
        "    pathToTrainTuples = os.path.join ( COCOPath, \"train_tuples.npy\" )\r\n",
        "    pathToValTuples = os.path.join ( COCOPath, \"val_tuples.npy\") \r\n",
        "\r\n",
        "    # TEST DATASET RELATED\r\n",
        "    pathToTest = None \r\n",
        "\r\n",
        "    # OUTPUTS\r\n",
        "    rootOutPath = os.path.join ( PROJECT_FOLDER, \"outputs\")\r\n",
        "    if not os.path.exists ( rootOutPath ):\r\n",
        "        os.mkdir ( rootOutPath ) \r\n",
        "\r\n",
        "    #------------------\r\n",
        "    # TRAININIG RELATED\r\n",
        "    #------------------\r\n",
        "    resumeTraining = True\r\n",
        "    netName = \"net-7\"\r\n",
        "    modelWeightsPath = os.path.join ( rootOutPath, \"weights-\" + netName + \".pt\") \r\n",
        "\r\n",
        "    trainLossesPath = os.path.join ( rootOutPath, \"train-losses-\" + netName + \".npy\") \r\n",
        "    valLossesPath = os.path.join ( rootOutPath, \"val-losses-\" + netName + \".npy\") \r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "#testing\r\n",
        "# globalVars = GlobalVars()\r\n",
        "# print(globalVars.PROJECT_FOLDER) "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjV8X5wd1PaB"
      },
      "source": [
        "# DataLoaders Synthetic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl-e36Cg9VcI"
      },
      "source": [
        "globalVars = GlobalVars() "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL1Fs56Z8xqm"
      },
      "source": [
        "# needed to import codes from drive.\r\n",
        "import sys\r\n",
        "sys.path.append ( globalVars.PROJECT_FOLDER )\r\n",
        "\r\n",
        "# import Synthetic Data functionality \r\n",
        "from importlib import reload \r\n",
        "import data_synthetic; reload ( data_synthetic )\r\n",
        "from data_synthetic import SyntheticData\r\n",
        "\r\n",
        "# import torch Dataloader\r\n",
        "from torch.utils.data import DataLoader  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M90EvckK8abl"
      },
      "source": [
        "class DataIteratorSynthetic:\r\n",
        "\r\n",
        "    def __init__(self, globalVars ):\r\n",
        "        self.globalVars = globalVars \r\n",
        "        self.val_dataloader = None\r\n",
        "        self.train_dataloader = None \r\n",
        "\r\n",
        "    def trainDataLoader(self,\r\n",
        "                        n_of_tuples = 200_000,\r\n",
        "                        train_batch_size = 16,\r\n",
        "                        num_workers = 4 ):\r\n",
        "\r\n",
        "        d_train = SyntheticData( path_to_COCO_file = self.globalVars.trainUnZipFilePath,\r\n",
        "                                 path_to_tuples = self.globalVars.pathToTrainTuples,\r\n",
        "                                 path_to_CDNET = self.globalVars.CDNETPath,\r\n",
        "                                 mode = \"train\",\r\n",
        "                                 n_of_tuples = 200_000,\r\n",
        "                                 cache_into_memory = False,\r\n",
        "                                data_mode = \"folder\"\r\n",
        "                                )\r\n",
        "        \r\n",
        "        self.train_dataloader = DataLoader(  d_train, batch_size = train_batch_size , shuffle = True, num_workers = num_workers )\r\n",
        "        return self.train_dataloader \r\n",
        "    \r\n",
        "    def valDataLoader(self, \r\n",
        "                      n_of_tuples = 5_000,\r\n",
        "                      val_batch_size = 8,\r\n",
        "                      num_workers = 2\r\n",
        "                      ):\r\n",
        "        d_val = SyntheticData (path_to_COCO_file = self.globalVars.valUnZipFilePath,\r\n",
        "                               path_to_tuples = self.globalVars.pathToValTuples,\r\n",
        "                               path_to_CDNET = globalVars.CDNETPath,\r\n",
        "                               mode = \"val\",\r\n",
        "                               n_of_tuples = n_of_tuples,\r\n",
        "                               cache_into_memory = False,\r\n",
        "                               data_mode = \"folder\"\r\n",
        "                               )\r\n",
        "        self.val_dataloader = DataLoader(  d_val, batch_size = val_batch_size , shuffle = True, num_workers = num_workers )\r\n",
        "        return self.val_dataloader \r\n",
        "    \r\n",
        "    def checkShapes(self):\r\n",
        "        if self.train_dataloader is no\r\n",
        "\r\n",
        "    def trainDataLoader(self):\r\n",
        "        pass "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "makEzvAgBzh9",
        "outputId": "0703305e-844c-4a69-f84d-5a476c74ed7f"
      },
      "source": [
        "dataiterator = DataIteratorSynthetic(globalVars=globalVars)\r\n",
        "# trainLoader = dataiterator.trainDataLoader()\r\n",
        "valLoader = dataiterator.valDataLoader() \r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/32 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tuples loaded from file /content/drive/My Drive/Projects/BGS/COCO/val_tuples.npy\n",
            "loading data from disk... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [10:02<00:00, 18.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done.\n",
            "\n",
            "                there are total masks = 52181 images\n",
            "                and from COCO there are total images = 5000\n",
            "                we are generating : 5000 tuples of two random images from COCO images.\n",
            "               \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLPxg5lUE6Yf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLaJ2fVGe1Ge"
      },
      "source": [
        "\n",
        "\n",
        "# create two datasets : train and validation \n",
        "def getDataLoader( train_batch_size = 16, val_batch_size = 8, unit_Test = True ):\n",
        "    \n",
        "    # train \n",
        "    d_train = SyntheticData (path_to_COCO_file = trainUnZipFilePath,\n",
        "                             path_to_tuples = pathToTrainTuples,\n",
        "                             path_to_CDNET = CDNETPath,\n",
        "                             mode = \"train\",\n",
        "                             n_of_tuples = 200_000,\n",
        "                             cache_into_memory = False\n",
        "                             ) \n",
        "    d_val = SyntheticData ( path_to_COCO_file = validationUnZipFilePath,\n",
        "                           path_to_tuples = pathToValTuples,\n",
        "                           path_to_CDNET = CDNETPath,\n",
        "                           mode = \"validation\",\n",
        "                           n_of_tuples = 5_000,\n",
        "                           cache_into_memory = False\n",
        "                           ) \n",
        "    if unit_Test:\n",
        "        inputImage, backgroundImage, mask = d_train[0]\n",
        "        print(f'''\n",
        "                train dataset\n",
        "                -------------\n",
        "                size : {len(d_train)}\n",
        "                inputImage shape : { inputImage.shape , inputImage.dtype }\n",
        "                backgroundImageShape : { backgroundImage.shape, backgroundImage.dtype}\n",
        "                mask shape : { mask.shape,mask.dtype }\n",
        "              ''') \n",
        "        \n",
        "        inputImage, backgroundImage, mask = d_val[0]\n",
        "        print(f'''\n",
        "                Validation dataset\n",
        "                -------------\n",
        "                size : {len(d_val)}\n",
        "                inputImage shape : { inputImage.shape, inputImage.dtype}\n",
        "                backgroundImageShape : { backgroundImage.shape,backgroundImage.dtype}\n",
        "                mask shape : { mask.shape, mask.dtype}\n",
        "              ''') \n",
        "        \n",
        "    train_dataloader = DataLoader(  d_train, batch_size = train_batch_size , shuffle = True, num_workers = 4 ) \n",
        "    val_dataloader = DataLoader ( d_val, batch_size = val_batch_size, shuffle = True , num_workers = 4 )\n",
        "\n",
        "    if unit_Test:\n",
        "\n",
        "        for i, ( inputImage, backgroundImage, mask ) in enumerate( train_dataloader ):\n",
        "            print(f'''\n",
        "                    train data loader\n",
        "                    -------------\n",
        "                    inputImage shape : { inputImage.shape, inputImage.dtype}\n",
        "                    backgroundImageShape : { backgroundImage.shape,backgroundImage.dtype}\n",
        "                    mask shape : { mask.shape, mask.dtype}\n",
        "              ''') \n",
        "            break \n",
        "\n",
        "        \n",
        "        for i, ( inputImage, backgroundImage, mask ) in enumerate( val_dataloader ):\n",
        "            print(f'''\n",
        "                    val data loader\n",
        "                    -------------\n",
        "                    inputImage shape : { inputImage.shape, inputImage.dtype}\n",
        "                    backgroundImageShape : { backgroundImage.shape,backgroundImage.dtype}\n",
        "                    mask shape : { mask.shape, mask.dtype}\n",
        "              ''') \n",
        "            break \n",
        "\n",
        "    return train_dataloader, val_dataloader  "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVyqvFoPLyN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8295a39-09ea-432d-efda-69acde351c5d"
      },
      "source": [
        "import torch.nn as nn \n",
        "import time \n",
        "import sys \n",
        "import os \n",
        "from importlib import reload \n",
        "import io \n",
        "import torch.optim as optim \n",
        "import numpy as np"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLztpTFFwGgU"
      },
      "source": [
        "#---------------------------------------------------------------\n",
        "# reload these in colab for change update without runtime restart\n",
        "#----------------------------------------------------------------\n",
        "import data_synthetic; reload ( data_synthetic )\n",
        "import FCNN; reload ( FCNN )\n",
        "import loss; reload ( loss )\n",
        "import utils; reload ( utils )\n",
        "from utils import *\n",
        "\n",
        "from data_synthetic import SyntheticData \n",
        "from FCNN import Net \n",
        "from loss import Jaccard \n",
        "import cv2"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqtFoB2TOrT-"
      },
      "source": [
        "#-------------\n",
        "# checkpoint\n",
        "#-------------\n",
        "import os \n",
        "import numpy as np \n",
        "\n",
        "def checkpoint( net, train_losses: list or 'np array', val_losses : list or 'np array ', verbose = True) -> None:\n",
        "    # save weights\n",
        "    if net is not None:\n",
        "        torch.save( net.state_dict(), model_weights_path )\n",
        "\n",
        "    # save train losses \n",
        "    if train_losses is not None:\n",
        "        np.save(train_losses_path, np.array ( train_losses ) ) \n",
        "\n",
        "    # save val losses \n",
        "    if val_losses is not None:\n",
        "        np.save(val_losses_path, np.array ( val_losses ) ) \n",
        "        \n",
        "    if verbose:\n",
        "        print(f\"\\n --checkpoint created for {net_name}\") \n",
        "\n",
        "# checkpoint ( None, [1], [1] )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XjvE6n6g3D9"
      },
      "source": [
        "train_dataloader, val_dataloader = getDataLoader ( train_batch_size  = 16, val_batch_size = 8 ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyCXwgXzxzx6"
      },
      "source": [
        "class DeviceManager:\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "        print(f\"device available : {self.device}\")\r\n",
        "    \r\n",
        "    def __call__(self):\r\n",
        "        return self.device \r\n",
        "\r\n",
        "class NetManager:\r\n",
        "\r\n",
        "    def __init__(self, net_name, model_weights_path ):\r\n",
        "\r\n",
        "        self.net = Net()\r\n",
        "        self.device = DeviceManager()\r\n",
        "        self.net.to ( self.device() )\r\n",
        "\r\n",
        "        self.model_weights_path = model_weights_path \r\n",
        "    \r\n",
        "    def loadCheckpoint(self):\r\n",
        "        flag = net.load_state_dict ( torch.load ( self.model_weights_path, map_location= torch.device( self.device() ) ) ) \r\n",
        "        print(flag)\r\n",
        "\r\n",
        "    def createCheckpoint(self):\r\n",
        "        torch.save( self.net.state_dict(), self.model_weights_path )\r\n",
        "    \r\n",
        "    def getNetSummary(self):\r\n",
        "        print ( self.net ) \r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdr9b6DBObP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "9d44d793-cebc-4c6a-c550-7e9c5c282783"
      },
      "source": [
        "device_manager = DeviceManager()\n",
        "net_manager = NetManager(net_name = net_name, model_weights_path = model_weights_path )\n",
        "if resume_training:\n",
        "    net_manager.loadCheckpoint() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f1ba4bbdda9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeviceManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnet_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_weights_path\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresume_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d81238cf52ab>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"device available : {self.device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoDrwOcTjfIR"
      },
      "source": [
        "#----------------------------\n",
        "# optimizer and loss function \n",
        "#----------------------------\n",
        "criterion = Jaccard( filterROI = False, smooth = 1000 ) \n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), \n",
        "                       lr=0.001, \n",
        "                       betas=(0.9, 0.999), \n",
        "                       eps=1e-08, \n",
        "                       weight_decay=0, \n",
        "                       amsgrad=False\n",
        "                       )"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAmb0Y9gf-JE"
      },
      "source": [
        "#-------------\n",
        "# validation\n",
        "#-------------\n",
        "from tqdm import tqdm \n",
        "\n",
        "if resume_training:\n",
        "    val_losses = list ( np.load ( val_losses_path ) ) \n",
        "else:\n",
        "    val_losses = [ 0 ] \n",
        "\n",
        "@torch.no_grad()\n",
        "def validate():\n",
        "    net.eval() \n",
        "    eval_run_loss = 0 \n",
        "    tqdm_test_iter = tqdm ( val_dataloader, position = 0, leave = True )\n",
        "\n",
        "    for i, data in enumerate ( tqdm_test_iter ): \n",
        "       \n",
        "        inputImages, backgroundImages, masks = data \n",
        "        inputs = torch.cat( [backgroundImages, inputImages], axis = 1 )\n",
        "        \n",
        "        inputs = inputs.to (device) \n",
        "        masks = masks.to(device)\n",
        "        outputs = net( inputs )\n",
        "        try:\n",
        "            temp_l = criterion( y_true = masks, y_pred = outputs )\n",
        "        except:\n",
        "            print(f'''\n",
        "                    inputs : {inputs.shape}\n",
        "                    masks : {masks.shape}\n",
        "                    outputs : {outputs.shape}\n",
        "                    ''')\n",
        "            raise ( Exception (\"loss calc error\"))\n",
        "\n",
        "        eval_run_loss += temp_l.detach().cpu().numpy() \n",
        "        tqdm_test_iter.set_description(f\"tloss:{np.round(eval_run_loss / (i+1), decimals=2)}\")  \n",
        "\n",
        "        if i % 100 == 0:\n",
        "            #--------------------\n",
        "            # show the outputs\n",
        "            #--------------------\n",
        "            true_mask = masks.cpu().squeeze().numpy() \n",
        "            indices_not_roi = np.where ( true_mask == 0.85 )\n",
        "\n",
        "            pred_mask = outputs.cpu().detach().numpy()\n",
        "            \n",
        "            # post process each prediction via median filter of size 9 \n",
        "            for i in range ( pred_mask.shape[0] ):\n",
        "                pred_mask[i] = ( cv2.medianBlur( np.uint8 ( pred_mask[i] * 255 ) , 9 ) / 255.0 ) \n",
        "\n",
        "            pred_mask [ pred_mask >= 0.5 ] = 1.0 \n",
        "            pred_mask [ pred_mask  < 0.5 ] = 0.0 \n",
        "            pred_mask [ indices_not_roi ] = 0.85 \n",
        "\n",
        "            # background and raw image\n",
        "            inputs = inputs.cpu().numpy() \n",
        "            bg = inputs[:, :3, :, :]; bg = bg.transpose ( (0, 2, 3, 1) ); bg = bg.squeeze() \n",
        "            raw = inputs[:, 3:, :, :]; raw = raw.transpose ( (0, 2, 3, 1) ); raw = raw.squeeze() \n",
        "\n",
        "            # save\n",
        "            # saving to root_out_path/Images/subfolderName/prefix-postfix.png\n",
        "            # postfix = len(files) already present in the \"subfolderName\" \n",
        "            show(matrices = [ bg, raw, true_mask, pred_mask ], \n",
        "                title = f\"validation at epoch {len(val_losses)}\",\n",
        "                descriptions = None,\n",
        "                root_out_path = root_out_path,\n",
        "                prefix = None,\n",
        "                subfolderName = \"validation\",\n",
        "                epochNum = f\"{len(val_losses)}\",\n",
        "                show_plots = False \n",
        "                )\n",
        "\n",
        "    # update the val losses\n",
        "    val_losses.append( eval_run_loss / ( i + 1 ) )\n",
        "    net.train()  "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk6B0KjMqmp0"
      },
      "source": [
        "# validate() \n",
        "# checkpoint(net, train_losses = None, val_losses = val_losses, verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQOCmDUpk480"
      },
      "source": [
        "# import utils; reload ( utils )\n",
        "# from utils import *\n",
        "# validate() \n",
        "\n",
        "# print ( len(val_losses)) \n",
        "# print ( len(train_losses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9tKN9Lak6J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9db843a-d0b2-4a6d-c91b-5e9832f7716c"
      },
      "source": [
        "#--------------\n",
        "# train \n",
        "#--------------\n",
        "from IPython.display import clear_output \n",
        "\n",
        "from tqdm import tqdm \n",
        "num_epochs = 100\n",
        "\n",
        "if resume_training:\n",
        "    train_losses = list ( np.load ( train_losses_path ) )\n",
        "else:\n",
        "    train_losses = [ 0 ]\n",
        "\n",
        "for epoch in range ( num_epochs ) :  # loop over the dataset multiple times\n",
        "    print(\"\\n\\n----------------------\")\n",
        "    print(f\"epoch : { len(train_losses)}\")\n",
        "    running_loss = 0.0 \n",
        "\n",
        "    tqdm_iter =  tqdm (  train_dataloader , position = 0, leave = True )\n",
        "    for i, data in enumerate ( tqdm_iter ) :\n",
        "        # if i == 1: break\n",
        "        # data is a list of [inputs, labels]\n",
        "        inputImages, backgroundImages, masks = data \n",
        "        inputs = torch.cat( [backgroundImages, inputImages], axis = 1 )\n",
        "        # print(f\"inputs shape : { inputs.shape }, masks shape : {masks.shape}\")\n",
        "        inputs = inputs.to(device)\n",
        "        masks =  masks.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net( inputs.double() )\n",
        "        loss = criterion(y_true = masks, y_pred = outputs )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update loss \n",
        "        running_loss += loss.cpu().detach().numpy()\n",
        "        tqdm_iter.set_description (f\"rloss: { np.round( running_loss/(i+1), decimals=2) }\")\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            checkpoint(net, None, None, verbose = False) \n",
        "\n",
        "            #----------------------------------------\n",
        "            # show results of the last batch trained. \n",
        "            #----------------------------------------\n",
        "            # true mask \n",
        "            true_mask = masks.cpu().numpy()\n",
        "            indices_not_roi = np.where ( true_mask == 0.85 ) # handiling the not roi region in pred mask \n",
        "\n",
        "            # predicted mask \n",
        "            pred_mask = outputs.cpu().detach().numpy()\n",
        "            \n",
        "            # post process each prediction via median filter of size 9 \n",
        "            for i in range ( pred_mask.shape[0] ):\n",
        "                pred_mask[i] = cv2.medianBlur( np.uint8 ( pred_mask[i] * 255 ) , 9 ) / 255.0 \n",
        "\n",
        "            pred_mask [pred_mask >= 0.5 ] = 1.0       # thresholding of 0.5 \n",
        "            pred_mask [ pred_mask < 0.5 ] = 0.0 \n",
        "            pred_mask [ indices_not_roi ] = 0.85 \n",
        "\n",
        "            # bg and raw \n",
        "            inputs = inputs.cpu().numpy() \n",
        "            # bg \n",
        "            bg = inputs[:,:3,:,:].transpose((0, 2,3, 1))\n",
        "            # raw \n",
        "            raw = inputs[:,3:, :, :].transpose((0, 2, 3, 1)) \n",
        "\n",
        "            show(matrices = [bg, raw, true_mask, pred_mask],\n",
        "                 title = f\"train at epoch-{len(train_losses)}\",\n",
        "                 descriptions = None,\n",
        "                 root_out_path = root_out_path,\n",
        "                 prefix = None,\n",
        "                 subfolderName = \"train\",\n",
        "                 epochNum = f\"{len(train_losses)}\",\n",
        "                 show_plots = False\n",
        "                )\n",
        "            \n",
        "    #------------------------\n",
        "    #------------------------\n",
        "    \n",
        "    # update train loss \n",
        "    train_losses.append ( running_loss / (i+1) )\n",
        "\n",
        "    #----------------\n",
        "    # run validation\n",
        "    #----------------\n",
        "    validate() \n",
        "\n",
        "    #------------------\n",
        "    # plot loss graphs \n",
        "    #------------------\n",
        "    plt.plot(train_losses, label = \"train_loss\")\n",
        "    plt.plot(val_losses, label = \"val_loss\")\n",
        "    plt.legend(); plt.xlabel(\"epoch num\"); plt.ylabel(\"loss\"); plt.title ( f\"loss graph epoch : {len(train_losses)}\")\n",
        "    plt.savefig ( os.path.join ( root_out_path,\"Images\", f\"loss_graph_{len(train_losses)}.png\" ) ) \n",
        "    plt.show() \n",
        "\n",
        "    #summary\n",
        "    print(f'''\n",
        "        average train loss ( last 3 ): { np.round ( train_losses[-3:], decimals= 2 ) } \n",
        "        average val loss ( last 3 ): { np.round (val_losses[-3:], decimals= 2) }\n",
        "    ''') \n",
        "    # break \n",
        "    checkpoint(net, train_losses, val_losses, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/12500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----------------------\n",
            "epoch : 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCMgVOP3waCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9f9092-8bc2-40b0-df84-c9bc26d756c7"
      },
      "source": [
        "# for i in range(2):\n",
        "#     val_losses.pop()\n",
        "#     train_losses.pop()\n",
        "\n",
        "# print ( len(val_losses), len(train_losses) )\n",
        "checkpoint( net, train_losses, val_losses )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " --checkpoint created for net-7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_e_GPKdSj7w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4be03520-58f5-4caa-e7f5-576d17126c7d"
      },
      "source": [
        "plt.plot(train_losses, label = \"train_loss\")\n",
        "plt.plot(val_losses, label = \"val_loss\")\n",
        "plt.legend(); plt.xlabel(\"epoch num\"); plt.ylabel(\"loss\"); plt.title ( f\"loss graph epoch : {len(train_losses)}\")\n",
        "plt.savefig ( os.path.join ( root_out_path,\"Images\", f\"loss_graph_{len(train_losses)}.png\" ) ) \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQwZaAAsGOZ7"
      },
      "source": [
        "#---------------\n",
        "# test on images\n",
        "#---------------\n",
        "\n",
        "# while training the input hybrid of 6 channels\n",
        "# was composed by first 3 channels from background image\n",
        "# and last 3 channels from true imageṇ\n",
        "from skimage import io \n",
        "\n",
        "def read ( image_filename, background_filename ):\n",
        "    ''' read raw image and background image as input hybrid '''\n",
        "    bg = io.imread ( background_filename )\n",
        "    raw = io.imread ( image_filename )\n",
        "    \n",
        "    assert bg.ndim == 3 and raw.ndim == 3, \"input is supposed to be of 6 channels\"\n",
        "\n",
        "    # make channel first\n",
        "    bg = bg.transpose ( (2,0,1))\n",
        "    raw = raw.transpose ( (2,0,1))\n",
        "\n",
        "    # add batch axis and normalise and change dtype to float\n",
        "    bg = bg[np.new_axis, :] )  / 255.0   \n",
        "    raw = raw[np.new_axis, :] / 255.0 \n",
        "    \n",
        "    input_hybrid = torch.cat ( [ torch.tensor ( bg ), torch.tensor (raw) ], axis = 1 ) \n",
        "    return input_hybrid \n",
        "\n",
        "@torch.no_grad()\n",
        "def forward(inputs):\n",
        "    ''' returns a binary map'''\n",
        "    output = net(inputs)\n",
        "    output = output.detach().cpu().squeeze() \n",
        "\n",
        "    output[output >= 0.5 ] = 1\n",
        "    output[ output < 0.5 ] = 0 \n",
        "    return output \n",
        "\n",
        "\n",
        "from glob import glob \n",
        "import os \n",
        "\n",
        "def main( test_images_dir ):\n",
        "    '''\n",
        "    directory where test_images are stored\n",
        "    - contain a folder named background\n",
        "    - contain a folder names images \n",
        "    '''\n",
        "    background_folder = os.path.join ( test_images_dir, \"background\" )    \n",
        "    images_folder = os.path.join ( test_images_dir, \"images\")\n",
        "\n",
        "    bg_image_file  = glob ( background_folder + \"/*\")[0] \n",
        "    images = glob ( images_folder + \"/*\") \n",
        "\n",
        "    for image_file in images:\n",
        "        inputs = read ( image_file, bg_image_file ) \n",
        "        pred_mask = forward ( inputs )\n",
        "        image = io.imread ( image_file )\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma9WxqdQOhce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624a95e8-f925-43f6-b773-53977ffae984"
      },
      "source": [
        "#--------- time check at different input shapes\n",
        "#-----------------------------------------------\n",
        "\n",
        "def timeTest():\n",
        "    net.eval() \n",
        "    for s in [ 37, 224,]:\n",
        "        image = torch.rand(10, 6, s, s ).cuda()\n",
        "        tick = time.time()\n",
        "        out = net ( image )\n",
        "        tock = time.time()\n",
        "        print(f'''\n",
        "            time taken is : { tock - tick } \n",
        "            input size : {image.shape}\n",
        "            output size : {out.shape}\n",
        "\n",
        "            ''') \n",
        "    net.train() \n",
        "        \n",
        "timeTest()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "            time taken is : 0.003214597702026367 \n",
            "            input size : torch.Size([10, 6, 37, 37])\n",
            "            output size : torch.Size([10, 37, 37])\n",
            "\n",
            "            \n",
            "\n",
            "            time taken is : 0.0034532546997070312 \n",
            "            input size : torch.Size([10, 6, 224, 224])\n",
            "            output size : torch.Size([10, 224, 224])\n",
            "\n",
            "            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uRl3IO2FwZg"
      },
      "source": [
        "path = os.path.join ( root_out_path, \"1\", \"4\")\n",
        "os.makedirs ( path )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SExLjH9kF2rM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}